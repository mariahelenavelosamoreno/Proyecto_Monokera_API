<h1 align="center">Proyecto ETL para Spaceflight News API</h1>

#  Descripci√≥n del Proyecto

Este proyecto implementa un pipeline **ETL** para extraer datos de la API de **Spaceflight News** (art√≠culos y blogs), transformarlos y cargarlos en archivos CSV. El sistema est√° construido con **Python** y **Apache Airflow**, permitiendo ejecuciones programadas y monitoreo del flujo de datos.

# Objetivos

- Extraer hasta **1000 registros** de los endpoints `/articles` y `/blogs`  
- Transformar los campos de fecha al formato `YYYY/MM/DD`  
- Generar archivos **CSV con separador `;`**  
- Implementar validaciones de calidad de datos  
- Crear un pipeline **automatizado y reproducible**

#  Arquitectura

<img width="1880" height="1055" alt="Diagrama de Arquitectura ETL API_ Spaceflight News (2)" src="https://github.com/user-attachments/assets/8c06c5ab-128b-44f4-af91-2e6650eac9de" />
<p align="center"><em><strong>Figura 1. Diagrama de arquitectura del pipeline ETL para la API de Spaceflight News.</strong></em></p>

### Flujo principal:

1. **Extracci√≥n:** Consumo de API ‚Üí Datos RAW  
2. **Transformaci√≥n:** Limpieza y formateo ‚Üí Datos STAGING  
3. **Validaci√≥n:** Verificaci√≥n de calidad  
4. **Carga:** Generaci√≥n de archivos finales

### Modelo de Arquitectura por Capas (Medallion Architecture)

El flujo principal del pipeline sigue una estructura basada en la arquitectura Medallion, que organiza el tratamiento de los datos en capas sucesivas para mejorar su calidad, trazabilidad y gobernanza.

Actualmente, se implementan las siguientes capas:

- Capa RAW (Bronze): Corresponde a la extracci√≥n directa de datos desde la API de Spaceflight News. Esta capa almacena los datos en su forma m√°s cruda, sin transformaciones ni filtrado, y sirve como respaldo fiel del origen para trazabilidad y reprocesos. En este proyecto, esta capa se encuentra representada por la carpeta /raw.

- Capa STAGING (Silver): Representa los datos ya transformados y limpios. En esta etapa se realiza el formateo de fechas, normalizaci√≥n de tipos, eliminaci√≥n de nulos cr√≠ticos y otras transformaciones ligeras. El objetivo es dejar los datos listos para an√°lisis o pasos posteriores. Se materializa en la carpeta /staging.

- Capa GOLD (no implementada a√∫n): Esta capa normalmente contiene datos enriquecidos, consolidados o listos para ser consumidos por el negocio o herramientas de BI.
En este caso, a√∫n no se ha implementado debido a que no se cuenta con requerimientos espec√≠ficos del negocio sobre c√≥mo deben ser los datos finales ni qu√© m√©tricas o indicadores se desean construir. La construcci√≥n de esta capa depender√° directamente del conocimiento del dominio y de los objetivos anal√≠ticos definidos por los stakeholders. Esto puede incluir agregaciones, joins con otras fuentes o c√°lculos complejos espec√≠ficos para reportes.

### Componentes:

- **Airflow** (Orquestaci√≥n)  
- **Python** (Procesamiento)  
- **Great Expectations** (Validaci√≥n)  
- **Docker** (Entorno de ejecuci√≥n)

# Desarrollo T√©cnico del Pipeline ETL: An√°lisis, Construcci√≥n, Validaci√≥n y Despliegue

### Analisis preeliminar

Para comprender la estructura y limitaciones de la fuente de datos, realic√© una exploraci√≥n inicial utilizando Postman, una herramienta que facilita la visualizaci√≥n, prueba y an√°lisis de endpoints de APIs REST. A trav√©s de esta herramienta se evalu√≥ el comportamiento del endpoint /articles y /blogs de la API p√∫blica de Spaceflight News:

- La documentaci√≥n completa se encuentra disponible en:
https://api.spaceflightnewsapi.net/v4/docs

- Se identific√≥ que la API tiene un l√≠mite de 500 registros por solicitud, configurable a trav√©s del par√°metro _limit.

- A partir de este an√°lisis, se dise√±√≥ una l√≥gica de extracci√≥n basada en el uso de offset, de modo que se pueda obtener la totalidad de los datos disponibles en bloques de 500 sin duplicaci√≥n.


<img width="1368" height="932" alt="image" src="https://github.com/user-attachments/assets/1eb3f20b-9333-4e2b-a4f6-df8189daa509" />
<p align="center"><em><strong>Figura 2. Ejemplo de respuesta de la API Spaceflight Blogs News analizada con Postman.</strong></em></p>


<img width="1363" height="935" alt="image" src="https://github.com/user-attachments/assets/85ebf1e0-19a1-4c62-8a44-8d8a40284acc" />
<p align="center"><em><strong>Figura 3. Ejemplo de respuesta de la API Spaceflight Articles News analizada con Postman.</strong></em></p>


### Control de duplicados y continuidad
Para evitar traer datos repetidos entre ejecuciones, se implement√≥ un mecanismo de control de estado mediante archivos JSON (state/state_articles.json, state/state_blogs.json). Estos archivos almacenan el √∫ltimo offset procesado exitosamente, permitiendo que en cada nueva ejecuci√≥n del pipeline, el proceso de extracci√≥n contin√∫e desde el punto exacto en el que se detuvo anteriormente.

Adicionalmente, los datos se extraen en orden cronol√≥gico ascendente (de los m√°s antiguos a los m√°s recientes), asegurando un orden l√≥gico en la construcci√≥n del hist√≥rico y facilitando futuras estrategias de ingesta incremental por fecha (updatedAt).

### Prevenci√≥n de sobreescritura de archivos
Durante el proceso de carga en las capas RAW y STAGING, se implement√≥ una convenci√≥n de nombramiento de archivos basada en timestamp, utilizando el siguiente formato:

`datetime = pd.Timestamp.now().strftime('%Y-%m-%d_%H-%M-%S')`.

Esta estrategia garantiza que:

- Cada archivo generado tenga un nombre √∫nico.

- No se sobrescriban archivos si el pipeline se ejecuta m√°s de una vez en el mismo d√≠a.

- Se mantenga un hist√≥rico de ejecuciones, √∫til para auditor√≠a, backfills o comparaci√≥n entre versiones de datos.

### Exploraci√≥n estructurada en notebooks
Todo el desarrollo inicial se realiz√≥ en Jupyter Notebooks, disponibles en el repositorio como:

`api_articles.ipynb`

`api_blogs.ipynb`

Estos notebooks sirvieron como entorno de prototipado para validar la conexi√≥n con la API, analizar la estructura de los datos, detectar inconsistencias y dise√±ar la l√≥gica de extracci√≥n y transformaci√≥n antes de integrarla a los DAGs de Airflow.

### Decisi√≥n de mantener las fuentes separadas
Aunque los endpoints de articles y blogs comparten una estructura de columnas pr√°cticamente id√©ntica, se opt√≥ por manejar su procesamiento de forma separada desde el inicio. Esta decisi√≥n responde a las siguientes buenas pr√°cticas en ingenier√≠a de datos:

- Mantener independencia entre fuentes, facilitando trazabilidad y control por origen.

- Permitir la ejecuci√≥n paralela o desacoplada de los pipelines.

- Adaptarse a futuras reglas de negocio o validaciones espec√≠ficas para cada tipo de contenido.

## Diagn√≥stico del Dataset
Durante la exploraci√≥n se realizaron las siguientes evaluaciones:

- Valores nulos: Se detect√≥ que columnas cr√≠ticas como id, title y publishedAt no deben contener valores nulos, mientras que campos como summary o listas como launches y events pueden estar vac√≠os sin afectar la integridad del dato.

- Tipos de variables: Se confirm√≥ que los datos llegan como strings (para texto y fechas), enteros (para IDs) y listas (para relaciones). Se definieron transformaciones para convertir fechas al formato YYYY/MM/DD, validar consistencia en tipos y eliminar datos inconsistentes.

- Estructura de columnas: La estructura general incluye campos como id, title, summary, url, imageUrl, publishedAt, updatedAt, newsSite, launches, events, entre otros. Esta estructura es adecuada para construir una vista cronol√≥gica y tem√°tica del contenido informativo sobre misiones espaciales.

<p align="center"><img width="472" height="462" alt="image" src="https://github.com/user-attachments/assets/6252063e-eea2-4deb-b6d8-61cfe56b2e76" />
<p align="center"><em><strong>Figura 4. Estructura del DataFrame tras extracci√≥n de la API.</strong></em></p>


## Desarrollo de la l√≥gica del pipeline DAG
El pipeline ETL fue desarrollado inicialmente en los notebooks api_articles.ipynb y api_blogs.ipynb, donde se valid√≥ paso a paso la l√≥gica de extracci√≥n, transformaci√≥n y validaci√≥n de datos. Posteriormente, esta l√≥gica fue migrada a un DAG de Apache Airflow, permitiendo una ejecuci√≥n orquestada y programada del flujo completo.

- Para mantener la modularidad y trazabilidad del origen de datos, se dise√±aron dos pipelines separados: uno para blogs y otro para art√≠culos, a pesar de que ambos comparten la misma estructura de columnas. Esta separaci√≥n permite:

- Trazabilidad por fuente

- Validaciones diferenciadas

- Escalabilidad y mantenibilidad del sistema

- Posible ejecuci√≥n paralela

###DAG spaceflight_blogs_etl y spaceflight_articles_etl
Este DAG implementa un flujo ETL completo con cinco tareas principales:

- Start_ETL: Inicia el flujo y registra una marca temporal. Esto es √∫til para auditor√≠a y control de ejecuciones.

- Extract_and_save_raw_csv: Realiza la extracci√≥n de datos desde el endpoint de blogs utilizando paginaci√≥n (limit=500 y offset incremental). La l√≥gica se apoya en un archivo state/state_blogs.json que guarda el √∫ltimo offset procesado, lo cual evita duplicados en ejecuciones futuras. Los datos extra√≠dos se almacenan en formato CSV en la carpeta raw/, usando un timestamp en el nombre del archivo para prevenir sobreescrituras.
- Transform_and_save_staging_csv: aplica transformaciones esenciales como la eliminaci√≥n de duplicados, la estandarizaci√≥n del formato de fechas en las columnas published_at y updated_at, y la limpieza de campos textuales para asegurar uniformidad. Tambi√©n convierte los tipos de datos seg√∫n lo requerido. Finalmente, guarda el resultado en la carpeta staging/, utilizando un nombre de archivo con timestamp para evitar sobrescrituras y mantener trazabilidad entre ejecuciones.

- Validate_raw: Realiza validaciones de calidad sobre los datos en crudo (RAW) usando Great Expectations. Se verifica la presencia de columnas obligatorias, tipos de datos, unicidad de IDs y formato de fechas.

- Validate_staging Aplica una segunda capa de validaci√≥n sobre los datos transformados (STAGING), asegurando que las reglas de negocio se cumplan antes de su uso anal√≠tico.

- End_ETL: Marca el fin del flujo e imprime el timestamp final de ejecuci√≥n.

<img width="1123" height="262" alt="image" src="https://github.com/user-attachments/assets/bf7c34aa-5f22-45b4-8a35-293dd8d0a168" />
<p align="center"><em><strong>Figura 5. DAG de Airflow.</strong></em></p>

### Caracter√≠sticas clave del DAG

- Orquestaci√≥n declarativa: Cada tarea est√° conectada mediante operadores >>, lo que permite un seguimiento visual del flujo.

- Reintentos y tolerancia a fallos: Airflow permite configurar reintentos autom√°ticos para cada tarea en caso de error.

- XComs: Se utilizan para compartir resultados intermedios entre tareas, como el path del archivo CSV generado o el DataFrame en formato JSON.

- Validaci√≥n robusta: Ambas capas (RAW y STAGING) son validadas de manera independiente usando suites definidas en archivos JSON (expectations/).

- Modularidad: Las funciones que realizan cada paso est√°n separadas y documentadas, facilitando su reutilizaci√≥n y mantenimiento.

## Validaciones Implementadas
Para asegurar la calidad y confiabilidad de los datos a lo largo del pipeline ETL, se implementaron validaciones autom√°ticas utilizando la librer√≠a Great Expectations. Las validaciones se ejecutan externamente tras la descarga de los archivos CSV para conservar los datos originales, incluso si presentan errores. Esto evita que el pipeline elimine o modifique informaci√≥n valiosa, facilitando trazabilidad, control y decisiones informadas por parte del cliente. Estas validaciones se aplican en dos etapas clave del proceso:

- RAW: datos directamente extra√≠dos de la API, sin transformaciones.

- STAGING: datos ya transformados y listos para su consumo.

Cada conjunto de datos tiene su propia Expectation Suite definida en archivos JSON (raw.json y staging.json), y se ejecutan autom√°ticamente dentro del DAG de Airflow.

### Validaciones en la capa RAW

- No nulos en id :Se usa expect_column_values_to_not_be_null en la columna id, con mostly = 1.0, para asegurar que ning√∫n registro tenga id vac√≠o.
Esto es cr√≠tico ya que el campo id es clave para la trazabilidad y posterior validaci√≥n de duplicados.

- IDs √∫nicos : Se usa expect_column_values_to_be_unique, validando que no haya duplicados en los identificadores dentro del archivo crudo.
Esto garantiza que cada registro extra√≠do represente una entidad distinta en la fuente.

- Estas validaciones est√°n marcadas con stop_pipeline_on_failure = true en los metadatos, lo que significa que si se incumplen, el DAG se detiene para evitar procesar datos inconsistentes.

### Validaciones en la capa STAGING

- No nulos en id: Se repite la validaci√≥n para confirmar que el proceso de transformaci√≥n no haya introducido valores vac√≠os.

- IDs √∫nicos: Asegura que no se generaron duplicados durante el procesamiento.

- Formato de fecha (published_at y updated_at): Se valida que ambas columnas de fecha respeten el formato %Y/%m/%d usando expect_column_values_to_match_strftime_format. Estas fechas fueron transformadas desde el formato original durante la etapa de STAGING. En lugar de sobrescribir las columnas originales, se opt√≥ por crear columnas auxiliares (published_datetime, updated_datetime) para conservar los datos crudos y facilitar auditor√≠as o trazabilidad.

- Valores esperados en featured: Se utiliza expect_column_values_to_be_in_set para asegurar que el campo booleano featured contenga solo los valores true o false, evitando errores de tipo o registros mal formateados.

### Uso de Great Expectations
- Permite definir reglas declarativas, reutilizables y expl√≠citas.

- Facilita el an√°lisis autom√°tico de errores y generaci√≥n de reportes.

- Se integra f√°cilmente con Airflow y pandas, sin necesidad de herramientas externas.

## Retos T√©cnicos

- **Manejo de paginaci√≥n:**  La API de Spaceflight News impone un l√≠mite de 500 registros por solicitud. Para poder extraer m√°s datos sin omitir informaci√≥n, se implement√≥ una l√≥gica de paginaci√≥n controlada mediante `offset`, acumulando los resultados de forma iterativa hasta alcanzar el n√∫mero deseado o agotar la fuente.

- **Validaci√≥n robusta con Great Expectations:**  Se definieron dos etapas de validaci√≥n: una al finalizar la extracci√≥n (RAW) y otra despu√©s de la transformaci√≥n (STAGING). Esto permite detectar errores tanto en el origen como en la l√≥gica de procesamiento, mejorando la calidad y confiabilidad de los datos entregados.

- **Despliegue del entorno con Docker:**  Se opt√≥ por usar **Docker** para garantizar la portabilidad del entorno y evitar los problemas comunes al instalar **Apache Airflow directamente en Windows**, como conflictos con dependencias, virtualenvs o errores en la inicializaci√≥n del scheduler. Docker permiti√≥ encapsular toda la configuraci√≥n del proyecto (Airflow, dependencias, rutas y vol√∫menes) en contenedores reproducibles, facilitando la ejecuci√≥n en cualquier sistema operativo y asegurando coherencia entre entornos de desarrollo y producci√≥n.

## üîß Configuraci√≥n del Entorno con Docker

Para asegurar portabilidad, aislamiento de dependencias y facilitar la ejecuci√≥n del pipeline en cualquier entorno, se utiliz√≥ **Docker** como contenedor principal para la arquitectura de Airflow.

Se cre√≥ una configuraci√≥n personalizada con los siguientes componentes:

### üìÑ `Dockerfile`

Este archivo define la imagen base del entorno de ejecuci√≥n, e incluye:

- Instalaci√≥n de **dependencias espec√≠ficas** como `pandas`, `requests`, `great_expectations`, y otras necesarias para el funcionamiento del pipeline.
- Configuraci√≥n del entorno de trabajo y copia de los scripts ETL.

### docker-compose.yml
Este archivo orquesta los servicios necesarios para ejecutar Apache Airflow en contenedores aislados. Se incluyen:

airflow-webserver

airflow-scheduler

airflow-worker

postgres como backend

redis para el broker de mensajes

airflow-init para inicializar el entorno

- Algunos par√°metros clave:

Montaje de vol√∫menes locales (./dags, ./logs, ./plugins)

Exposici√≥n de puertos (por ejemplo, 8080 para la interfaz web de Airflow)

Persistencia de datos en vol√∫menes de Docker


##  Mejoras Futuras

- **Almacenamiento en la nube (S3 o Data Warehouse):**   Migrar los archivos CSV almacenados localmente a buckets en la nube como Amazon S3, Google Cloud Storage o un Data Warehouse. Esto facilitar√° el acceso distribuido, el versionamiento, la escalabilidad y la integraci√≥n con sistemas anal√≠ticos o BI.

- **Monitoreo y alertas autom√°ticas:**  Integrar herramientas de monitoreo (como Slack o correo electr√≥nico) para notificar fallos, validaciones fallidas o tareas omitidas. Esto mejorar√≠a la visibilidad operativa y permitir√≠a una respuesta m√°s r√°pida ante errores.

- **Backfilling controlado:**  Incorporar l√≥gica de backfilling que permita reprocesar datos hist√≥ricos sin interferir con los flujos diarios. Esto ser√≠a √∫til para nuevos modelos, correcci√≥n de errores o auditor√≠as.

- **Ingesta completa inicial (historical load):**  Dise√±ar un DAG alternativo capaz de extraer todo el hist√≥rico de datos disponibles mediante paginaci√≥n completa. Esta l√≥gica podr√≠a implementarse como un flujo one-time, aprovechando un bucle con `offset` hasta agotar los registros.
# C√≥digo para Extracci√≥n Hist√≥rica Completa (ETL)

`def Extract_full_historical_load(**kwargs):
    ti = kwargs['ti']
    config = DEFAULT_CONFIG
    
    print("Iniciando extracci√≥n HIST√ìRICA completa (one-time)")

    # 1. Cargar estado previo (si existe)
    state_file = config["STATE_FILE"]
    try:
        with open(state_file, 'r') as f:
            state = json.load(f)
            last_offset = state.get("last_offset", 0)
            print(f"Offset inicial desde state.json: {last_offset}")
    except (FileNotFoundError, json.JSONDecodeError):
        last_offset = 0
        print("No se encontr√≥ state.json. Iniciando desde offset 0")

    # 2. Configuraci√≥n de la extracci√≥n
    current_offset = last_offset  # Comienza desde el √∫ltimo offset conocido
    data = []
    extraction_active = True
    max_records = float('inf')  # Sin l√≠mite de registros

    # 3. Extracci√≥n paginada completa
    while extraction_active and len(data) < max_records:
        try:
            blogs = fetch_blogs(config["BASE_URL"], config["LIMIT"], current_offset)
            
            if not blogs:  # Fin de los datos
                print(" No hay m√°s registros en la API")
                extraction_active = False
                break
                
            # Agregar datos y actualizar offset
            received = len(blogs)
            data.extend(blogs)
            current_offset += received
            
            # Actualizar estado en cada iteraci√≥n (para resiliencia)
            with open(state_file, 'w') as f:
                json.dump({"last_offset": current_offset}, f)
            
            print(f" Lote recibido: {received} registros | Offset acumulado: {current_offset}")

            # Peque√±o delay para evitar rate-limiting
            time.sleep(0.3)  
            
        except Exception as e:
            print(f" Error en extracci√≥n hist√≥rica: {str(e)}")
            # Conserva el √∫ltimo offset v√°lido
            with open(state_file, 'w') as f:
                json.dump({"last_offset": current_offset}, f)
            raise

    # 4. Guardado de datos brutos
    if data:
        raw_df = pd.json_normalize(data)
        os.makedirs(config["RAW_DIR"], exist_ok=True)
        timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
        output_file = f"{config['RAW_DIR']}/FULL_HISTORICAL_{timestamp}_blogs_raw.csv"
        
        try:
            raw_df.to_csv(output_file, sep=';', index=False)
            print(f"Extracci√≥n hist√≥rica completada. Total registros: {len(data)}")
            print(f"Archivo generado: {output_file}")

            # Actualizar estado final
            with open(state_file, 'w') as f:
                json.dump({"last_offset": current_offset, "last_execution": timestamp}, f)

            # Compartir metadatos
            ti.xcom_push(key='historical_raw_file', value=output_file)
            ti.xcom_push(key='total_records', value=len(data))
            
        except Exception as e:
            print(f"Error al guardar CSV hist√≥rico: {str(e)}")
            raise
    else:
        print("No se encontraron registros nuevos en la API")

    # 5. Log del estado final
    print("\n--- ESTADO FINAL ---")
    print(f"√öltimo offset procesado: {current_offset}")
    print(f"Registros totales extra√≠dos: {len(data)}")`

- **Control incremental con `updated_at`:**  Implementar una l√≥gica de ingesta que no dependa de un l√≠mite fijo de registros, sino que utilice la columna `updated_at` para consultar solo los datos nuevos o actualizados desde la √∫ltima ejecuci√≥n exitosa. Esto permitir√≠a mantener el dataset actualizado en tiempo real.

- **Gesti√≥n de ejecuciones sin nuevos datos:**  A√±adir una condici√≥n al DAG para que, si no se detectan nuevos registros por extraer, simplemente se salte la ejecuci√≥n sin lanzar errores, y registre un log que indique "sin cambios".
---

### Tiempos de Desarrollo

| Etapa                              | Tiempo estimado |
|------------------------------------|-----------------|
| Dise√±o de arquitectura             | 2 horas         |
| Implementaci√≥n ETL b√°sico          | 4 horas         |
| Integraci√≥n con Airflow            | 3 horas         |
| Validaciones y manejo de errores   | 5 horas         |
| Configuraci√≥n del ambiente Docker  | 5 horas         |
| Documentaci√≥n y ajustes finales    | 4 horas         |


### Recursos
- [Documentaci√≥n oficial de Spaceflight News API](https://api.spaceflightnewsapi.net/v4/docs/)
- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Great Expectations Documentation](https://docs.greatexpectations.io/docs/home/)
- [Docker Docs](https://docs.docker.com/)
