# Proyecto ETL para Spaceflight News API

##  Descripci√≥n del Proyecto

Este proyecto implementa un pipeline **ETL** para extraer datos de la API de **Spaceflight News** (art√≠culos y blogs), transformarlos y cargarlos en archivos CSV. El sistema est√° construido con **Python** y **Apache Airflow**, permitiendo ejecuciones programadas y monitoreo del flujo de datos.

---

## Objetivos

- Extraer hasta **1000 registros** de los endpoints `/articles` y `/blogs`  
- Transformar los campos de fecha al formato `YYYY/MM/DD`  
- Generar archivos **CSV con separador `;`**  
- Implementar validaciones de calidad de datos  
- Crear un pipeline **automatizado y reproducible**

---

##  Arquitectura

[Ver Diagrama de Arquitectura en Lucidchart](https://lucid.app/lucidchart/988d7156-d739-4fa9-be01-a13a76fd7a4e/view)

### Flujo principal:

1. **Extracci√≥n:** Consumo de API ‚Üí Datos RAW  
2. **Transformaci√≥n:** Limpieza y formateo ‚Üí Datos STAGING  
3. **Validaci√≥n:** Verificaci√≥n de calidad  
4. **Carga:** Generaci√≥n de archivos finales

### Componentes:

- **Airflow** (Orquestaci√≥n)  
- **Python** (Procesamiento)  
- **Great Expectations** (Validaci√≥n)  
- **Docker** (Entorno de ejecuci√≥n)

---

##  Configuraci√≥n

### Requisitos previos

- Docker y Docker Compose  
- Python 3.10+  
- WSL2 (para usuarios Windows)

üîç Validaciones Implementadas
Se incluyen checks para:

Existencia de columnas obligatorias (id, title, etc.)

Formato correcto de fechas

Valores no nulos en campos cr√≠ticos

Tipos de datos esperados

Unicidad de IDs

üß† Retos T√©cnicos
Manejo de paginaci√≥n: la API tiene l√≠mite de 500 registros por request, se implement√≥ paginaci√≥n con control de offset.

Estructura variable de datos: algunos campos opcionales requieren validaci√≥n condicional.

Validaci√≥n robusta: uso de Great Expectations en etapas RAW y STAGING.

Paralelizaci√≥n: validaciones se ejecutan en paralelo con transformaciones para eficiencia.

### Tiempos de Desarrollo

| Etapa                              | Tiempo estimado |
|------------------------------------|-----------------|
| Dise√±o de arquitectura             | 2 horas         |
| Implementaci√≥n ETL b√°sico          | 4 horas         |
| Integraci√≥n con Airflow            | 3 horas         |
| Validaciones y manejo de errores   | 5 horas         |
| Configuraci√≥n del ambiente Docker  | 5 horas         |
| Documentaci√≥n y ajustes finales    | 4 horas         |

### Mejoras Futuras
- Almacenamiento en la nube (S3 o GCS): Migrar los archivos locales a buckets en la nube para facilitar la escalabilidad y acceso distribuido.
- Monitoreo con alertas y m√©tricas: Integrar herramientas como Prometheus + Grafana o Airflow + Slack para notificar fallos y medir rendimiento.
- Pruebas unitarias autom√°ticas: Incorporar testing en los scripts de transformaci√≥n y validaci√≥n con pytest.
- CI/CD con GitHub Actions: Automatizar pruebas, validaciones y despliegues del pipeline.
- Backfilling: Implementar mecanismos para reprocesar hist√≥ricos de forma controlada.
- Ingesta completa inicial en el primer DAG: Agregar una l√≥gica alternativa para realizar una descarga completa de los datos hist√≥ricos. Se propone usar un bucle con paginaci√≥n autom√°tica para obtener todos los registros disponibles.
- Control incremental por updated_at: En lugar de fijar un l√≠mite est√°tico de registros (como 1000), implementar un mecanismo que consulte la √∫ltima fecha (updated_at) insertada por el DAG del d√≠a anterior y descargue solo los registros nuevos o actualizados:

### Recursos
- [Documentaci√≥n oficial de Spaceflight News API](https://api.spaceflightnewsapi.net/v4/docs/)
- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Great Expectations Documentation](https://docs.greatexpectations.io/docs/home/)
- [Docker Docs](https://docs.docker.com/)
