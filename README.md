# Proyecto ETL para Spaceflight News API

##  DescripciÃ³n del Proyecto

Este proyecto implementa un pipeline **ETL** para extraer datos de la API de **Spaceflight News** (artÃ­culos y blogs), transformarlos y cargarlos en archivos CSV. El sistema estÃ¡ construido con **Python** y **Apache Airflow**, permitiendo ejecuciones programadas y monitoreo del flujo de datos.

---

## Objetivos

- Extraer hasta **1000 registros** de los endpoints `/articles` y `/blogs`  
- Transformar los campos de fecha al formato `YYYY/MM/DD`  
- Generar archivos **CSV con separador `;`**  
- Implementar validaciones de calidad de datos  
- Crear un pipeline **automatizado y reproducible**

---

##  Arquitectura

[Ver Diagrama de Arquitectura en Lucidchart](https://lucid.app/lucidchart/988d7156-d739-4fa9-be01-a13a76fd7a4e/view)

### Flujo principal:

1. **ExtracciÃ³n:** Consumo de API â†’ Datos RAW  
2. **TransformaciÃ³n:** Limpieza y formateo â†’ Datos STAGING  
3. **ValidaciÃ³n:** VerificaciÃ³n de calidad  
4. **Carga:** GeneraciÃ³n de archivos finales

### Componentes:

- **Airflow** (OrquestaciÃ³n)  
- **Python** (Procesamiento)  
- **Great Expectations** (ValidaciÃ³n)  
- **Docker** (Entorno de ejecuciÃ³n)

---

##  ConfiguraciÃ³n

### Requisitos previos

- Docker y Docker Compose  
- Python 3.10+  
- WSL2 (para usuarios Windows)

### InstalaciÃ³n

Clona el repositorio:



Inicia Airflow con Docker:

bash
Copiar
Editar
docker-compose up -d
Accede a la interfaz web:

URL: http://localhost:8080

Usuario: admin

ContraseÃ±a: admin (o consultar standalone_admin_password.txt)

ğŸ“‚ Estructura del Proyecto
perl
Copiar
Editar
spaceflight-etl/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ articles_etl.py           # Pipeline para artÃ­culos
â”‚   â””â”€â”€ blogs_etl.py              # Pipeline para blogs
â”œâ”€â”€ expectations/                 # Definiciones de validaciÃ³n
â”‚   â”œâ”€â”€ raw_articles.json
â”‚   â”œâ”€â”€ staging_articles.json
â”‚   â”œâ”€â”€ raw_blogs.json
â”‚   â””â”€â”€ staging_blogs.json
â”œâ”€â”€ state/                        # Tracking de ejecuciones
â”‚   â””â”€â”€ state_articles.json
â”œâ”€â”€ raw/                          # Datos brutos
â”œâ”€â”€ staging/                      # Datos transformados
â”œâ”€â”€ docs/                         # DocumentaciÃ³n
â”œâ”€â”€ docker-compose.yaml           # ConfiguraciÃ³n de Airflow
â””â”€â”€ requirements.txt              # Dependencias Python
ğŸš€ EjecuciÃ³n
Los DAGs estÃ¡n programados para ejecutarse diariamente.
Para ejecutarlos manualmente:

Accede a la interfaz web de Airflow

Encuentra los DAGs spaceflight_articles_etl y spaceflight_blogs_etl

ActÃ­valos y haz clic en "Trigger DAG"

ğŸ” Validaciones Implementadas
Se incluyen checks para:

Existencia de columnas obligatorias (id, title, etc.)

Formato correcto de fechas

Valores no nulos en campos crÃ­ticos

Tipos de datos esperados

Unicidad de IDs

ğŸ§  Retos TÃ©cnicos
Manejo de paginaciÃ³n: la API tiene lÃ­mite de 500 registros por request, se implementÃ³ paginaciÃ³n con control de offset.

Estructura variable de datos: algunos campos opcionales requieren validaciÃ³n condicional.

ValidaciÃ³n robusta: uso de Great Expectations en etapas RAW y STAGING.

ParalelizaciÃ³n: validaciones se ejecutan en paralelo con transformaciones para eficiencia.

**Tiempos de Desarrollo**

| Etapa                              | Tiempo estimado |
|------------------------------------|-----------------|
| DiseÃ±o de arquitectura             | 2 horas         |
| ImplementaciÃ³n ETL bÃ¡sico          | 4 horas         |
| IntegraciÃ³n con Airflow            | 3 horas         |
| Validaciones y manejo de errores   | 5 horas         |
| ConfiguraciÃ³n del ambiente Docker  | 5 horas         |
| DocumentaciÃ³n y ajustes finales    | 4 horas         |

###Mejoras Futuras
- Almacenamiento en la nube (S3 o GCS): Migrar los archivos locales a buckets en la nube para facilitar la escalabilidad y acceso distribuido.
- Monitoreo con alertas y mÃ©tricas: Integrar herramientas como Prometheus + Grafana o Airflow + Slack para notificar fallos y medir rendimiento.
- Pruebas unitarias automÃ¡ticas: Incorporar testing en los scripts de transformaciÃ³n y validaciÃ³n con pytest.
- CI/CD con GitHub Actions: Automatizar pruebas, validaciones y despliegues del pipeline.
- Backfilling: Implementar mecanismos para reprocesar histÃ³ricos de forma controlada.
- Ingesta completa inicial en el primer DAG: Agregar una lÃ³gica alternativa para realizar una descarga completa de los datos histÃ³ricos. Se propone usar un bucle con paginaciÃ³n automÃ¡tica para obtener todos los registros disponibles.
- Control incremental por updated_at: En lugar de fijar un lÃ­mite estÃ¡tico de registros (como 1000), implementar un mecanismo que consulte la Ãºltima fecha (updated_at) insertada por el DAG del dÃ­a anterior y descargue solo los registros nuevos o actualizados:

### Recursos
- [DocumentaciÃ³n oficial de Spaceflight News API](https://api.spaceflightnewsapi.net/v4/docs/)
- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Great Expectations Documentation](https://docs.greatexpectations.io/docs/home/)
- [Docker Docs](https://docs.docker.com/)


