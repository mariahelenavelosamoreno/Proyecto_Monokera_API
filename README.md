# Proyecto ETL para Spaceflight News API

##  DescripciÃ³n del Proyecto

Este proyecto implementa un pipeline **ETL** para extraer datos de la API de **Spaceflight News** (artÃ­culos y blogs), transformarlos y cargarlos en archivos CSV. El sistema estÃ¡ construido con **Python** y **Apache Airflow**, permitiendo ejecuciones programadas y monitoreo del flujo de datos.

---

## Objetivos

- Extraer hasta **1000 registros** de los endpoints `/articles` y `/blogs`  
- Transformar los campos de fecha al formato `YYYY/MM/DD`  
- Generar archivos **CSV con separador `;`**  
- Implementar validaciones de calidad de datos  
- Crear un pipeline **automatizado y reproducible**

---

##  Arquitectura

[Ver Diagrama de Arquitectura en Lucidchart](https://lucid.app/lucidchart/988d7156-d739-4fa9-be01-a13a76fd7a4e/view)

### Flujo principal:

1. **ExtracciÃ³n:** Consumo de API â†’ Datos RAW  
2. **TransformaciÃ³n:** Limpieza y formateo â†’ Datos STAGING  
3. **ValidaciÃ³n:** VerificaciÃ³n de calidad  
4. **Carga:** GeneraciÃ³n de archivos finales

### Componentes:

- **Airflow** (OrquestaciÃ³n)  
- **Python** (Procesamiento)  
- **Great Expectations** (ValidaciÃ³n)  
- **Docker** (Entorno de ejecuciÃ³n)

---

##  ConfiguraciÃ³n

### Requisitos previos

- Docker y Docker Compose  
- Python 3.10+  
- WSL2 (para usuarios Windows)

### InstalaciÃ³n

Clona el repositorio:

```bash
git clone https://github.com/tu-usuario/spaceflight-etl.git
cd spaceflight-etl

Inicia Airflow con Docker:

bash
Copiar
Editar
docker-compose up -d
Accede a la interfaz web:

URL: http://localhost:8080

Usuario: admin

ContraseÃ±a: admin (o consultar standalone_admin_password.txt)

ğŸ“‚ Estructura del Proyecto
perl
Copiar
Editar
spaceflight-etl/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ articles_etl.py           # Pipeline para artÃ­culos
â”‚   â””â”€â”€ blogs_etl.py              # Pipeline para blogs
â”œâ”€â”€ expectations/                 # Definiciones de validaciÃ³n
â”‚   â”œâ”€â”€ raw_articles.json
â”‚   â”œâ”€â”€ staging_articles.json
â”‚   â”œâ”€â”€ raw_blogs.json
â”‚   â””â”€â”€ staging_blogs.json
â”œâ”€â”€ state/                        # Tracking de ejecuciones
â”‚   â””â”€â”€ state_articles.json
â”œâ”€â”€ raw/                          # Datos brutos
â”œâ”€â”€ staging/                      # Datos transformados
â”œâ”€â”€ docs/                         # DocumentaciÃ³n
â”œâ”€â”€ docker-compose.yaml           # ConfiguraciÃ³n de Airflow
â””â”€â”€ requirements.txt              # Dependencias Python
ğŸš€ EjecuciÃ³n
Los DAGs estÃ¡n programados para ejecutarse diariamente.
Para ejecutarlos manualmente:

Accede a la interfaz web de Airflow

Encuentra los DAGs spaceflight_articles_etl y spaceflight_blogs_etl

ActÃ­valos y haz clic en "Trigger DAG"

ğŸ” Validaciones Implementadas
Se incluyen checks para:

Existencia de columnas obligatorias (id, title, etc.)

Formato correcto de fechas

Valores no nulos en campos crÃ­ticos

Tipos de datos esperados

Unicidad de IDs

ğŸ§  Retos TÃ©cnicos
Manejo de paginaciÃ³n: la API tiene lÃ­mite de 500 registros por request, se implementÃ³ paginaciÃ³n con control de offset.

Estructura variable de datos: algunos campos opcionales requieren validaciÃ³n condicional.

ValidaciÃ³n robusta: uso de Great Expectations en etapas RAW y STAGING.

ParalelizaciÃ³n: validaciones se ejecutan en paralelo con transformaciones para eficiencia.

â±ï¸ Tiempos de Desarrollo
Etapa	Tiempo estimado
DiseÃ±o de arquitectura	2 horas
ImplementaciÃ³n ETL bÃ¡sico	4 horas
IntegraciÃ³n con Airflow	3 horas
Validaciones y manejo de errores	5 horas
DocumentaciÃ³n y ajustes finales	2 horas

ğŸ“ˆ Mejoras Futuras
ğŸ’¾ Almacenamiento en la nube (S3 o GCS)

ğŸ“Š Monitoreo con alertas y mÃ©tricas

ğŸ§ª Pruebas unitarias automÃ¡ticas

ğŸ” CI/CD con GitHub Actions

ğŸ“† Backfilling para reprocesar histÃ³ricos

ğŸ“š Recursos
DocumentaciÃ³n oficial de Spaceflight News API

Apache Airflow Documentation

Great Expectations Documentation

yaml
Copiar
Editar

